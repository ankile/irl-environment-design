{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr_dir = os.path.split(os.getcwd())[1]\n",
    "if curr_dir != \"irl-environment-design\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.utils.constants import ParamTuple\n",
    "from src.utils.make_environment import (\n",
    "    transition_matrix,\n",
    "    Environment,\n",
    "    insert_walls_into_T,\n",
    ")\n",
    "\n",
    "height = 7\n",
    "width = 7\n",
    "rewards = np.zeros((height, width))\n",
    "wall_states = np.zeros((height, width))\n",
    "\n",
    "rewards[6,0] = 1\n",
    "rewards[6,6] = 3\n",
    "\n",
    "# rewards[5,0] = -1\n",
    "# rewards[4,0] = -1\n",
    "\n",
    "rewards = rewards.flatten()\n",
    "\n",
    "goal_states = np.where(rewards > 0)[0]\n",
    "\n",
    "wall_states[2,0] = 1\n",
    "# wall_states[4,1] = 1\n",
    "\n",
    "wall_states = wall_states.flatten()\n",
    "wall_states = np.where(wall_states > 0)[0]\n",
    "\n",
    "agent_p = 0.9\n",
    "agent_gamma = 0.7\n",
    "p_true=1\n",
    "\n",
    "true_params = ParamTuple(agent_p, agent_gamma, rewards)\n",
    "\n",
    "wall_states = [14]\n",
    "\n",
    "T_true = transition_matrix(height, width, p=p_true, absorbing_states=goal_states)\n",
    "T_True = insert_walls_into_T(T=T_true, wall_indices=wall_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration to find fixed point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def soft_q_iteration_torch(\n",
    "    R: torch.Tensor,  # R is a one-dimensional tensor with shape (n_states,)\n",
    "    T_agent: torch.Tensor,\n",
    "    gamma: float,\n",
    "    beta: float,  # Inverse temperature parameter for the softmax function\n",
    "    tol: float = 1e-6,\n",
    ") -> torch.Tensor:\n",
    "    n_states, n_actions, _ = T_agent.shape\n",
    "    V = torch.zeros(n_states)\n",
    "    Q = torch.zeros((n_states, n_actions))\n",
    "    policy = torch.zeros((n_states, n_actions))\n",
    "\n",
    "    while True:\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                # Calculate the Q-value for action a in state s\n",
    "                Q[s, a] = R[s] + gamma * torch.dot(T_agent[s, a], V)\n",
    "\n",
    "        # Apply softmax to get a probabilistic policy\n",
    "        # max_Q = torch.max(Q, axis=1, keepdim=True)[0]\n",
    "        exp_Q = torch.exp(beta * (Q))  # Subtract max_Q for numerical stability\n",
    "        # exp_Q = torch.exp(beta * (Q - max_Q))  # Subtract max_Q for numerical stability\n",
    "\n",
    "        policy = exp_Q / torch.sum(exp_Q, axis=1, keepdim=True)\n",
    "\n",
    "        # Calculate the value function V using the probabilistic policy\n",
    "        V_new = torch.sum(policy * Q, axis=1)\n",
    "\n",
    "        # Check for convergence\n",
    "        if torch.max(torch.abs(V - V_new)) < tol:\n",
    "            break\n",
    "\n",
    "        V = V_new\n",
    "\n",
    "    return Q, V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_star, V_star, policy_star = soft_q_iteration_torch(R = torch.tensor(rewards, dtype=torch.float32), T_agent = torch.tensor(T_True, dtype=torch.float32), gamma = agent_gamma, beta = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Bellman Operator for fixed point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_bellman_update(R, gamma, T, Q):\n",
    "    return R + gamma * torch.matmul(T, torch.log(torch.sum(torch.exp(Q), axis=1)))\n",
    "\n",
    "def soft_bellman_fp(R, gamma, T, Q):\n",
    "    return soft_bellman_update(R, gamma, T, Q) - Q\n",
    "\n",
    "def soft_q_iteration_torch(R, gamma, T, Q_init=None, tol=1e-6):\n",
    "    if Q_init is None:\n",
    "        Q_init = torch.zeros(R.shape[0], 4)\n",
    "\n",
    "    Q = Q_init\n",
    "    \n",
    "    while True:\n",
    "        Q_new = soft_bellman_update(R, gamma,T, Q)\n",
    "        if torch.max(torch.abs(Q - Q_new)) < tol:\n",
    "            break\n",
    "        Q = Q_new\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = torch.tensor(rewards, dtype=torch.float32, requires_grad=True)\n",
    "R_stretched = R.unsqueeze(1).repeat(1,4) # Stretch R to match the shape of Q, e.g. we switch from R(s) to R(s,a).\n",
    "\n",
    "Q_star = soft_q_iteration_torch(R_stretched, agent_gamma, torch.tensor(T_True, dtype=torch.float32), Q_init=None, tol=1e-6)\n",
    "\n",
    "beta = 1\n",
    "exp_Q = torch.exp(beta * (Q_star))  # Subtract max_Q for numerical stability\n",
    "\n",
    "policy = exp_Q / torch.sum(exp_Q, axis=1, keepdim=True)\n",
    "\n",
    "# Calculate the value function V using the probabilistic policy\n",
    "V_new = torch.sum(policy * Q_star, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Derivative of Value Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_grad = torch.autograd.Variable(R_stretched, requires_grad=True)\n",
    "\n",
    "psi = soft_bellman_fp(R = R_grad, gamma=agent_gamma, T=torch.tensor(T_True, dtype=torch.float32), Q = Q_star)\n",
    "\n",
    "psi.backward(torch.ones_like(psi), inputs=[R])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000,\n",
       "        4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000,\n",
       "        4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000,\n",
       "        4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000,\n",
       "        4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 4.0000,\n",
       "        4.0000, 4.0000, 4.0000, 4.0000])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_grad = - R.grad\n",
    "psi_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
