{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr_dir = os.path.split(os.getcwd())[1]\n",
    "if curr_dir != \"irl-environment-design\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.utils.make_environment import (\n",
    "    transition_matrix,\n",
    "    Environment,\n",
    "    insert_walls_into_T,\n",
    ")\n",
    "\n",
    "from src.utils.constants import GenParamTuple# candidate_environments_args[\"n_environments\"] = 50\n",
    "\n",
    "\n",
    "np.set_printoptions(linewidth=160, precision=2)\n",
    "\n",
    "from src.utils.environment_design import EnvironmentDesign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make true environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Setup\n",
    "## 0.2 Setup the environment\n",
    "N, M = 7,7\n",
    "\n",
    "# TODO describe world\n",
    "agent_R = np.zeros((N, M))\n",
    "agent_R[N-1, 0] = 1\n",
    "agent_R[N-1, M-1] = 3\n",
    "\n",
    "\n",
    "\n",
    "# Start by making the agent we want to learn the policy of\n",
    "agent_gamma = 0.6\n",
    "p_true=0.7\n",
    "\n",
    "agent_R = agent_R.flatten()\n",
    "goal_states = np.where(agent_R != 0)[0]\n",
    "\n",
    "\n",
    "wall_states = [14] #TODO: why do we need this\n",
    "\n",
    "T_true = transition_matrix(N, M, p=p_true, absorbing_states=goal_states)\n",
    "T_True = insert_walls_into_T(T=T_true, wall_indices=wall_states)\n",
    "\n",
    "\n",
    "def custom_transition_func(p):\n",
    "\n",
    "    _T = transition_matrix(N=7, M=7, p=p, absorbing_states=goal_states)\n",
    "    _T = insert_walls_into_T(T=_T, wall_indices=wall_states)\n",
    "    return _T\n",
    "\n",
    "def custom_gamma_func(gamma):\n",
    "    return gamma\n",
    "\n",
    "def custom_reward_func(*reward_func):\n",
    "    return reward_func\n",
    "\n",
    "#Create parameter ranges\n",
    "resolution = 15\n",
    "p_range = np.linspace(0.7, 0.9, resolution)\n",
    "gamma_range = np.linspace(0.6, 0.9, resolution)\n",
    "\n",
    "\n",
    "gamma_range = gamma_range.reshape(1, resolution)\n",
    "p_range = p_range.reshape(1, resolution)\n",
    "\n",
    "true_params = GenParamTuple(T = custom_transition_func(p_true), gamma=agent_gamma, R=agent_R)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "big_small = Environment(\n",
    "    N=N,\n",
    "    M=M,\n",
    "    reward_function = custom_reward_func,\n",
    "    transition_function=custom_transition_func,\n",
    "    gamma = custom_gamma_func,\n",
    "    wall_states=wall_states,\n",
    "    start_state=0,\n",
    "    goal_states=goal_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated parameter mesh of shape:  (15, 15)\n"
     ]
    }
   ],
   "source": [
    "env_design = EnvironmentDesign(base_environment=big_small, \n",
    "                               user_params=true_params, \n",
    "                               learn_what = [\"gamma\", \"T\"],\n",
    "                               parameter_ranges_R=None,\n",
    "                               parameter_ranges_gamma=gamma_range,\n",
    "                               parameter_ranges_T=p_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started episode 0.\n",
      "Finished episode 0.\n",
      "Started episode 1.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.6863480080274913, 0.7910756448235351]\n",
      "Computed Region of Interest. Size = 0.48\n",
      "Finished BM Search. Entropy: 1.081770401476498.\n",
      "Finished episode 1.\n",
      "Started episode 2.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.663002573362093, 0.7830207654894656]\n",
      "Computed Region of Interest. Size = 0.35\n",
      "Finished BM Search. Entropy: 1.0154436148434798.\n",
      "Finished episode 2.\n",
      "Started episode 3.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.6512693865309769, 0.7711833934343407]\n",
      "Computed Region of Interest. Size = 0.26\n",
      "Finished BM Search. Entropy: 0.9229936905441962.\n",
      "Finished episode 3.\n",
      "Started episode 4.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.6449203530801908, 0.7586531432643634]\n",
      "Computed Region of Interest. Size = 0.2\n",
      "Finished BM Search. Entropy: 0.7856008801438331.\n",
      "Finished episode 4.\n",
      "Started episode 5.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.6381197827564064, 0.7480590201642726]\n",
      "Computed Region of Interest. Size = 0.15\n",
      "Finished BM Search. Entropy: 0.6816244336247836.\n",
      "Finished episode 5.\n",
      "Started episode 6.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.6329599381011327, 0.7408823101029393]\n",
      "Computed Region of Interest. Size = 0.12\n",
      "Finished BM Search. Entropy: 0.666278442414676.\n",
      "Finished episode 6.\n",
      "Started episode 7.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.627852165721783, 0.7367848930577788]\n",
      "Computed Region of Interest. Size = 0.09\n",
      "Finished BM Search. Entropy: 0.5623351446188083.\n",
      "Finished episode 7.\n",
      "Started episode 8.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.6241219567468702, 0.7313771220444055]\n",
      "Computed Region of Interest. Size = 0.07\n",
      "Finished BM Search. Entropy: 0.6931471805599453.\n",
      "Finished episode 8.\n",
      "Started episode 9.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.6210967283307802, 0.7270329956644853]\n",
      "Computed Region of Interest. Size = 0.06\n",
      "Finished BM Search. Entropy: 0.6901856760188042.\n",
      "Finished episode 9.\n",
      "Started episode 10.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.6210628049427432, 0.7209312798386002]\n",
      "Computed Region of Interest. Size = 0.05\n",
      "Finished BM Search. Entropy: 0.6554817739013927.\n",
      "Finished episode 10.\n",
      "Started episode 11.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.6155716477574413, 0.7219318088181409]\n",
      "Computed Region of Interest. Size = 0.04\n",
      "Finished BM Search. Entropy: 0.6869615765973234.\n",
      "Finished episode 11.\n",
      "Started episode 12.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.6164508186677258, 0.7158780181273044]\n",
      "Computed Region of Interest. Size = 0.04\n",
      "Finished BM Search. Entropy: 0.5623351446188083.\n",
      "Finished episode 12.\n",
      "Started episode 13.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.6133760580606767, 0.7161563965075975]\n",
      "Computed Region of Interest. Size = 0.03\n",
      "Finished BM Search. Entropy: 0.6829081047004717.\n",
      "Finished episode 13.\n",
      "Started episode 14.\n",
      "Beginning calculation of log-likelihood. Calculating 225 samples.\n",
      "Mean Parameters: [0.615131446862298, 0.7184573543368511]\n",
      "Computed Region of Interest. Size = 0.03\n",
      "Finished BM Search. Entropy: 0.6365141682948128.\n",
      "Finished episode 14.\n"
     ]
    }
   ],
   "source": [
    "candidate_environments_args = {}\n",
    "candidate_environments_args[\"generate_how\"] = \"entropy_BM\"\n",
    "\n",
    "env_design.run_n_episodes(n_episodes = 15,\n",
    "                          candidate_environments_args=candidate_environments_args,\n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nuser parameters consist of\\n- parametrizations of reward, transition and gamma that user supplies\\n- ranges for the unknown parameters\\n- resolution of the grid search\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "user parameters consist of\n",
    "- parametrizations of reward, transition and gamma that user supplies\n",
    "- ranges for the unknown parameters\n",
    "- resolution of the grid search\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
