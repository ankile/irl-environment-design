{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr_dir = os.path.split(os.getcwd())[1]\n",
    "if curr_dir != \"irl-environment-design\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.utils.make_environment import (\n",
    "    transition_matrix,\n",
    "    Environment,\n",
    "    insert_walls_into_T,\n",
    ")\n",
    "\n",
    "from src.utils.constants import GenParamTuple# candidate_environments_args[\"n_environments\"] = 50\n",
    "\n",
    "\n",
    "np.set_printoptions(linewidth=160, precision=2)\n",
    "\n",
    "from src.utils.environment_design import EnvironmentDesign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make true environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cliff World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N, M = 4, 8\n",
    "\n",
    "# agent_gamma = np.array(0.7)\n",
    "# p_true= np.array(0.9)\n",
    "# reward_true = np.array(0.8)\n",
    "\n",
    "# agent_R = np.zeros((N, M))\n",
    "# agent_R[0,1] = -1\n",
    "# agent_R[0,2] = -1\n",
    "# agent_R[0,3] = -1\n",
    "# agent_R[0,4] = -1\n",
    "# agent_R[0,5] = -1\n",
    "# agent_R[0,6] = -1\n",
    "# agent_R[0,7] = reward_true\n",
    "# agent_R = agent_R.flatten()\n",
    "\n",
    "# goal_states = np.where(agent_R != 0)[0]\n",
    "\n",
    "# T = transition_matrix(N, M, p_true, absorbing_states=goal_states)\n",
    "\n",
    "# #Define custom functions to generate reward, transition and gamma.\n",
    "# def custom_transition_func(p):\n",
    "\n",
    "#     _T = transition_matrix(N=4, M=8, p=p, absorbing_states=goal_states)\n",
    "#     return _T\n",
    "\n",
    "# def custom_gamma_func(gamma):\n",
    "#     return gamma\n",
    "\n",
    "# def custom_reward_func(reward):\n",
    "#     agent_R = np.zeros((4, 8))\n",
    "#     agent_R[0,1] = -1\n",
    "#     agent_R[0,2] = -1\n",
    "#     agent_R[0,3] = -1\n",
    "#     agent_R[0,4] = -1\n",
    "#     agent_R[0,5] = -1\n",
    "#     agent_R[0,6] = -1\n",
    "#     agent_R[0,7] = reward\n",
    "#     return agent_R.flatten()\n",
    "\n",
    "# #Create parameter ranges\n",
    "# resolution = 15\n",
    "# p_range = np.linspace(0.7, 0.95, resolution)\n",
    "# gamma_range = np.linspace(0.5, 0.95, resolution)\n",
    "# R_range = np.linspace(0.7, 0.95, resolution)\n",
    "\n",
    "# gamma_range = gamma_range.reshape(1, resolution)\n",
    "# p_range = p_range.reshape(1, resolution)\n",
    "# R_range = R_range.reshape(1, resolution)\n",
    "\n",
    "\n",
    "\n",
    "# p_true = p_true.reshape(1, 1)\n",
    "# agent_gamma = agent_gamma.reshape(1, 1)\n",
    "# reward_true = reward_true.reshape(1, 1)\n",
    "\n",
    "\n",
    "\n",
    "# true_params = GenParamTuple(T = p_true, gamma=agent_gamma, R=reward_true)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cliff = Environment(\n",
    "#     N=N,\n",
    "#     M=M,\n",
    "#     reward_function = custom_reward_func,\n",
    "#     transition_function=custom_transition_func,\n",
    "#     gamma = custom_gamma_func,\n",
    "#     wall_states=[],\n",
    "#     start_state=0,\n",
    "#     goal_states=goal_states\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Setup\n",
    "N, M = 7,7\n",
    "\n",
    "\n",
    "\n",
    "# Start by making the agent we want to learn the parameters of\n",
    "agent_gamma = np.array(0.85)\n",
    "p_true= np.array(0.92)\n",
    "big_reward_true = np.array(0.7)\n",
    "\n",
    "agent_R = np.zeros((N, M))\n",
    "agent_R[N-1, 0] = 0.1\n",
    "agent_R[N-1, M-1] = big_reward_true\n",
    "agent_R = agent_R.flatten()\n",
    "\n",
    "goal_states = np.where(agent_R != 0)[0]\n",
    "\n",
    "\n",
    "# wall_states = [14] #TODO: why do we need this\n",
    "wall_states = []\n",
    "\n",
    "T_true = transition_matrix(N, M, p=p_true, absorbing_states=goal_states)\n",
    "T_True = insert_walls_into_T(T=T_true, wall_indices=wall_states)\n",
    "\n",
    "\n",
    "#Define custom functions to generate reward, transition and gamma.\n",
    "def custom_transition_func(p):\n",
    "\n",
    "    _T = transition_matrix(N=7, M=7, p=p, absorbing_states=goal_states)\n",
    "    _T = insert_walls_into_T(T=_T, wall_indices=wall_states)\n",
    "    return _T\n",
    "\n",
    "def custom_gamma_func(gamma):\n",
    "    return gamma\n",
    "\n",
    "def custom_reward_func(big_reward):\n",
    "    reward_func = np.zeros((N, M))\n",
    "    reward_func[N-1, 0] = 0.1\n",
    "    reward_func[N-1, M-1] = big_reward\n",
    "    return reward_func.flatten()\n",
    "\n",
    "\n",
    "#Create parameter ranges\n",
    "resolution = 20\n",
    "p_range = np.linspace(0.7, 0.95, resolution)\n",
    "gamma_range = np.linspace(0.7, 0.95, resolution)\n",
    "R_range = np.linspace(0.3, 0.95, resolution)\n",
    "\n",
    "\n",
    "gamma_range = gamma_range.reshape(1, resolution)\n",
    "p_range = p_range.reshape(1, resolution)\n",
    "R_range = R_range.reshape(1, resolution)\n",
    "\n",
    "p_true = p_true.reshape(1, 1)\n",
    "agent_gamma = agent_gamma.reshape(1, 1)\n",
    "big_reward_true = big_reward_true.reshape(1, 1)\n",
    "\n",
    "\n",
    "\n",
    "true_params = GenParamTuple(T = p_true, gamma=agent_gamma, R=big_reward_true)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "big_small = Environment(\n",
    "    N=N,\n",
    "    M=M,\n",
    "    reward_function = custom_reward_func,\n",
    "    transition_function=custom_transition_func,\n",
    "    gamma = custom_gamma_func,\n",
    "    wall_states=wall_states,\n",
    "    start_state=1,\n",
    "    goal_states=goal_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated parameter mesh of shape:  (20, 20)\n"
     ]
    }
   ],
   "source": [
    "env_design = EnvironmentDesign(base_environment=big_small, \n",
    "                               user_params=true_params, \n",
    "                               learn_what = [\"gamma\", \"R\"],\n",
    "                               parameter_ranges_R=R_range,\n",
    "                               parameter_ranges_gamma=gamma_range,\n",
    "                               parameter_ranges_T=p_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50696/1602460577.py:39: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  reward_func[N-1, M-1] = big_reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started episode 0.\n",
      "Finished episode 0.\n",
      "Started episode 1.\n",
      "Beginning calculation of log-likelihood. Calculating 400 samples.\n",
      "Mean Parameters: [0.6282447716143945, 0.8293779963067454]\n",
      "Computed Region of Interest. Size = 0.78\n",
      "Initialized Reward Function R: [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.1  0.   0.   0.   0.   0.   0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RDDRDDRRDRD': 0}, pidx2states={0: [1, 2, 9, 16, 17, 24, 31, 32, 33, 40, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  1\n",
      "Reward Function:  [ 0.   -0.   -0.    0.    0.    0.    0.    0.    0.   -0.    0.    0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.    0.    0.    0.   -0.    0.\n",
      "  0.    0.    0.    0.    0.   -0.   -0.   -0.    0.    0.    0.    0.    0.    0.   -0.   -0.    0.1   0.    0.    0.    0.    0.    0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DDDDRRRDRDR': 0}, pidx2states={0: [1, 8, 15, 22, 29, 30, 31, 32, 39, 40, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  2\n",
      "Reward Function:  [ 0.   -0.01 -0.    0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.    0.    0.   -0.   -0.   -0.    0.    0.    0.    0.   -0.    0.   -0.    0.\n",
      "  0.    0.    0.   -0.   -0.   -0.01 -0.01 -0.    0.    0.    0.    0.    0.   -0.   -0.01 -0.    0.1   0.    0.    0.    0.   -0.    0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RRDRDDRDDDR': 0}, pidx2states={0: [1, 2, 3, 10, 11, 18, 25, 26, 33, 40, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  3\n",
      "Reward Function:  [ 0.   -0.01 -0.01 -0.    0.    0.    0.    0.   -0.   -0.   -0.   -0.    0.    0.    0.   -0.   -0.   -0.   -0.    0.    0.    0.   -0.    0.   -0.   -0.\n",
      " -0.    0.    0.   -0.   -0.   -0.01 -0.01 -0.01  0.    0.    0.    0.    0.   -0.   -0.01 -0.    0.1   0.    0.    0.    0.   -0.01  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DDDRDDRRRRD': 0}, pidx2states={0: [1, 8, 15, 22, 23, 30, 37, 38, 39, 40, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  4\n",
      "Reward Function:  [ 0.   -0.01 -0.01 -0.    0.    0.    0.    0.   -0.01 -0.   -0.   -0.    0.    0.    0.   -0.01 -0.   -0.   -0.    0.    0.    0.   -0.01 -0.   -0.   -0.\n",
      " -0.    0.    0.   -0.   -0.01 -0.01 -0.01 -0.01  0.    0.    0.   -0.   -0.   -0.01 -0.01 -0.01  0.1   0.    0.    0.    0.   -0.01  0.61]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RRRDDRDDDDR': 0}, pidx2states={0: [1, 2, 3, 4, 11, 18, 19, 26, 33, 40, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  5\n",
      "Reward Function:  [ 0.   -0.01 -0.01 -0.01 -0.    0.    0.    0.   -0.01 -0.   -0.   -0.01  0.    0.    0.   -0.01 -0.   -0.   -0.01 -0.    0.    0.   -0.01 -0.   -0.   -0.\n",
      " -0.01  0.    0.   -0.   -0.01 -0.01 -0.01 -0.01  0.    0.    0.   -0.   -0.   -0.01 -0.02 -0.01  0.1   0.    0.    0.    0.   -0.01  0.61]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DRDRDRDDRRD': 0}, pidx2states={0: [1, 8, 9, 16, 17, 24, 25, 32, 39, 40, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  6\n",
      "Reward Function:  [ 0.   -0.02 -0.01 -0.01 -0.    0.    0.    0.   -0.01 -0.01 -0.   -0.01  0.    0.    0.   -0.01 -0.01 -0.01 -0.01 -0.    0.    0.   -0.01 -0.   -0.01 -0.01\n",
      " -0.01  0.    0.   -0.   -0.01 -0.01 -0.01 -0.01  0.    0.    0.   -0.   -0.   -0.01 -0.02 -0.01  0.1   0.    0.    0.    0.   -0.01  0.6 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RRRRDDDDRDD': 0}, pidx2states={0: [1, 2, 3, 4, 5, 12, 19, 26, 33, 34, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  7\n",
      "Reward Function:  [ 0.   -0.02 -0.01 -0.01 -0.01 -0.    0.    0.   -0.01 -0.01 -0.   -0.01 -0.    0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01  0.    0.   -0.01 -0.   -0.01 -0.01\n",
      " -0.01  0.    0.   -0.   -0.01 -0.01 -0.01 -0.01 -0.    0.    0.   -0.   -0.   -0.01 -0.02 -0.01  0.1   0.    0.    0.    0.   -0.01  0.6 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DDDDDRRRDRR': 0}, pidx2states={0: [1, 8, 15, 22, 29, 36, 37, 38, 39, 46, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  8\n",
      "Reward Function:  [ 0.   -0.02 -0.01 -0.01 -0.01 -0.    0.    0.   -0.01 -0.01 -0.   -0.01 -0.    0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01  0.    0.   -0.01 -0.   -0.01 -0.01\n",
      " -0.01  0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.    0.   -0.   -0.01 -0.01 -0.01 -0.02 -0.01  0.1   0.    0.    0.   -0.   -0.01  0.6 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RDRRRDRDDDD': 0}, pidx2states={0: [1, 2, 9, 10, 11, 12, 19, 20, 27, 34, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  9\n",
      "Reward Function:  [ 0.   -0.03 -0.01 -0.01 -0.01 -0.    0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01  0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.    0.   -0.01 -0.   -0.01 -0.01\n",
      " -0.01 -0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.   -0.01 -0.01 -0.01 -0.02 -0.02  0.1   0.    0.    0.   -0.   -0.01  0.59]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DDRDDDRDRRR': 0}, pidx2states={0: [1, 8, 15, 16, 23, 30, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  10\n",
      "Reward Function:  [ 0.   -0.03 -0.01 -0.01 -0.01 -0.    0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01  0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.    0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.01 -0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.   -0.01 -0.01 -0.01 -0.02 -0.02  0.1   0.    0.   -0.   -0.01 -0.02  0.59]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RDRDDRDDRDR': 0}, pidx2states={0: [1, 2, 9, 10, 17, 24, 25, 32, 39, 40, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  11\n",
      "Reward Function:  [ 0.   -0.03 -0.02 -0.01 -0.01 -0.    0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01  0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.    0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.01 -0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.   -0.01 -0.01 -0.02 -0.02 -0.02  0.1   0.    0.   -0.   -0.01 -0.02  0.58]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DDDDDDRRRRR': 0}, pidx2states={0: [1, 8, 15, 22, 29, 36, 43, 44, 45, 46, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  12\n",
      "Reward Function:  [ 0.   -0.04 -0.02 -0.01 -0.01 -0.    0.    0.   -0.02 -0.01 -0.01 -0.01 -0.01  0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.    0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.01 -0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.02 -0.02 -0.02  0.1  -0.   -0.   -0.01 -0.01 -0.02  0.58]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RRRRDRDDDDD': 0}, pidx2states={0: [1, 2, 3, 4, 5, 12, 13, 20, 27, 34, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  13\n",
      "Reward Function:  [ 0.   -0.04 -0.02 -0.01 -0.01 -0.01  0.    0.   -0.02 -0.01 -0.01 -0.01 -0.01 -0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.02 -0.02 -0.02  0.1  -0.   -0.   -0.01 -0.01 -0.02  0.58]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DRDDRDDRRRD': 0}, pidx2states={0: [1, 8, 9, 16, 23, 24, 31, 38, 39, 40, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  14\n",
      "Reward Function:  [ 0.   -0.04 -0.02 -0.01 -0.01 -0.01  0.    0.   -0.02 -0.01 -0.01 -0.01 -0.01 -0.    0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.02 -0.03 -0.02  0.1  -0.   -0.   -0.01 -0.01 -0.02  0.57]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RRRRRDDDDDD': 0}, pidx2states={0: [1, 2, 3, 4, 5, 6, 13, 20, 27, 34, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  15\n",
      "Reward Function:  [ 0.   -0.04 -0.02 -0.01 -0.01 -0.01 -0.    0.   -0.02 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.02 -0.03 -0.03  0.1  -0.   -0.   -0.01 -0.01 -0.02  0.57]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DRRDRDRDDDR': 0}, pidx2states={0: [1, 8, 9, 10, 17, 18, 25, 26, 33, 40, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  16\n",
      "Reward Function:  [ 0.   -0.05 -0.02 -0.01 -0.01 -0.01 -0.    0.   -0.02 -0.02 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01 -0.02 -0.01  0.   -0.01 -0.01 -0.01 -0.02 -0.03 -0.03  0.1  -0.   -0.   -0.01 -0.01 -0.03  0.56]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DDDDDDRRRRR': 0}, pidx2states={0: [1, 8, 15, 22, 29, 36, 43, 44, 45, 46, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  17\n",
      "Reward Function:  [ 0.   -0.05 -0.02 -0.01 -0.01 -0.01 -0.    0.   -0.03 -0.02 -0.01 -0.01 -0.01 -0.01  0.   -0.02 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01 -0.02 -0.01  0.   -0.01 -0.01 -0.01 -0.02 -0.03 -0.03  0.1  -0.01 -0.01 -0.01 -0.01 -0.03  0.56]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RRDRDRDDRDD': 0}, pidx2states={0: [1, 2, 3, 10, 11, 18, 19, 26, 33, 34, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  18\n",
      "Reward Function:  [ 0.   -0.05 -0.03 -0.02 -0.01 -0.01 -0.    0.   -0.03 -0.02 -0.01 -0.01 -0.01 -0.01  0.   -0.02 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01 -0.02 -0.02  0.   -0.01 -0.01 -0.01 -0.02 -0.03 -0.03  0.1  -0.01 -0.01 -0.01 -0.01 -0.03  0.56]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DDRDDRDRDRR': 0}, pidx2states={0: [1, 8, 15, 16, 23, 30, 31, 38, 39, 46, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  19\n",
      "Reward Function:  [ 0.   -0.06 -0.03 -0.02 -0.01 -0.01 -0.    0.   -0.03 -0.02 -0.01 -0.01 -0.01 -0.01  0.   -0.02 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01 -0.02 -0.02  0.   -0.01 -0.01 -0.01 -0.02 -0.03 -0.03  0.1  -0.01 -0.01 -0.01 -0.02 -0.03  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RRRRDDDDRDD': 0}, pidx2states={0: [1, 2, 3, 4, 5, 12, 19, 26, 33, 34, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  20\n",
      "Reward Function:  [ 0.   -0.06 -0.03 -0.02 -0.01 -0.01 -0.    0.   -0.03 -0.02 -0.01 -0.01 -0.01 -0.01  0.   -0.02 -0.01 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.02 -0.01  0.   -0.01 -0.01 -0.01 -0.01 -0.02 -0.02  0.   -0.01 -0.01 -0.01 -0.02 -0.03 -0.03  0.1  -0.01 -0.01 -0.01 -0.02 -0.03  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DRDDDDDRRRR': 0}, pidx2states={0: [1, 8, 9, 16, 23, 30, 37, 44, 45, 46, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  21\n",
      "Reward Function:  [ 0.   -0.06 -0.03 -0.02 -0.01 -0.01 -0.    0.   -0.03 -0.02 -0.01 -0.01 -0.01 -0.01  0.   -0.02 -0.02 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.02 -0.01  0.   -0.01 -0.01 -0.01 -0.01 -0.02 -0.02  0.   -0.01 -0.01 -0.01 -0.02 -0.03 -0.03  0.1  -0.01 -0.01 -0.01 -0.02 -0.04  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RRDDDDRRDRD': 0}, pidx2states={0: [1, 2, 3, 10, 17, 24, 31, 32, 33, 40, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  22\n",
      "Reward Function:  [ 0.   -0.06 -0.03 -0.02 -0.01 -0.01 -0.    0.   -0.03 -0.02 -0.02 -0.01 -0.01 -0.01  0.   -0.02 -0.02 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.02 -0.01  0.   -0.01 -0.01 -0.01 -0.01 -0.02 -0.02  0.   -0.01 -0.01 -0.01 -0.02 -0.03 -0.04  0.1  -0.01 -0.01 -0.01 -0.02 -0.04  0.54]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DRRRDDDDRRD': 0}, pidx2states={0: [1, 8, 9, 10, 11, 18, 25, 32, 39, 40, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  23\n",
      "Reward Function:  [ 0.   -0.07 -0.03 -0.02 -0.01 -0.01 -0.    0.   -0.04 -0.02 -0.02 -0.01 -0.01 -0.01  0.   -0.02 -0.02 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.02 -0.01  0.   -0.01 -0.01 -0.01 -0.02 -0.02 -0.02  0.   -0.01 -0.01 -0.01 -0.02 -0.04 -0.04  0.1  -0.01 -0.01 -0.01 -0.02 -0.04  0.54]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RRRRRDDDDDD': 0}, pidx2states={0: [1, 2, 3, 4, 5, 6, 13, 20, 27, 34, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  24\n",
      "Reward Function:  [ 0.   -0.07 -0.03 -0.03 -0.02 -0.01 -0.01  0.   -0.04 -0.02 -0.02 -0.01 -0.01 -0.01  0.   -0.02 -0.02 -0.01 -0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.01 -0.01\n",
      " -0.02 -0.01  0.   -0.01 -0.01 -0.01 -0.02 -0.02 -0.02  0.   -0.01 -0.01 -0.01 -0.02 -0.04 -0.05  0.1  -0.01 -0.01 -0.01 -0.02 -0.04  0.53]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DDDDDDRRRRR': 0}, pidx2states={0: [1, 8, 15, 22, 29, 36, 43, 44, 45, 46, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  25\n",
      "Reward Function:  [ 0.   -0.07 -0.03 -0.03 -0.02 -0.01 -0.01  0.   -0.04 -0.02 -0.02 -0.01 -0.01 -0.01  0.   -0.02 -0.02 -0.01 -0.01 -0.01 -0.01  0.   -0.02 -0.01 -0.01 -0.01\n",
      " -0.02 -0.01  0.   -0.01 -0.01 -0.01 -0.02 -0.02 -0.02  0.   -0.01 -0.01 -0.01 -0.02 -0.04 -0.05  0.1  -0.01 -0.01 -0.01 -0.02 -0.04  0.53]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RDDDDDRRDRR': 0}, pidx2states={0: [1, 2, 9, 16, 23, 30, 37, 38, 39, 46, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  26\n",
      "Reward Function:  [ 0.   -0.08 -0.04 -0.03 -0.02 -0.01 -0.01  0.   -0.04 -0.03 -0.02 -0.01 -0.01 -0.01  0.   -0.02 -0.02 -0.01 -0.01 -0.01 -0.01  0.   -0.02 -0.02 -0.01 -0.01\n",
      " -0.02 -0.01  0.   -0.01 -0.02 -0.01 -0.02 -0.02 -0.02  0.   -0.01 -0.01 -0.02 -0.03 -0.04 -0.05  0.1  -0.01 -0.01 -0.01 -0.02 -0.05  0.53]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DDDDDDRRRRR': 0}, pidx2states={0: [1, 8, 15, 22, 29, 36, 43, 44, 45, 46, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  27\n",
      "Reward Function:  [ 0.   -0.08 -0.04 -0.03 -0.02 -0.01 -0.01  0.   -0.04 -0.03 -0.02 -0.01 -0.01 -0.01  0.   -0.03 -0.02 -0.01 -0.01 -0.01 -0.01  0.   -0.02 -0.02 -0.01 -0.01\n",
      " -0.02 -0.01  0.   -0.02 -0.02 -0.01 -0.02 -0.02 -0.02  0.   -0.01 -0.01 -0.02 -0.03 -0.04 -0.05  0.1  -0.01 -0.01 -0.02 -0.03 -0.05  0.52]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RRRRDDDRDDD': 0}, pidx2states={0: [1, 2, 3, 4, 5, 12, 19, 26, 27, 34, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  28\n",
      "Reward Function:  [ 0.   -0.08 -0.04 -0.03 -0.02 -0.02 -0.01  0.   -0.04 -0.03 -0.02 -0.01 -0.01 -0.01  0.   -0.03 -0.02 -0.01 -0.01 -0.02 -0.01  0.   -0.02 -0.02 -0.01 -0.01\n",
      " -0.02 -0.01  0.   -0.02 -0.02 -0.01 -0.02 -0.02 -0.02  0.   -0.01 -0.01 -0.02 -0.03 -0.04 -0.05  0.1  -0.01 -0.01 -0.02 -0.03 -0.05  0.52]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DRRDDRDRDRD': 0}, pidx2states={0: [1, 8, 9, 10, 17, 24, 25, 32, 33, 40, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  29\n",
      "Reward Function:  [ 0.   -0.08 -0.04 -0.03 -0.02 -0.02 -0.01  0.   -0.04 -0.03 -0.02 -0.01 -0.01 -0.01  0.   -0.03 -0.02 -0.02 -0.01 -0.02 -0.01  0.   -0.02 -0.02 -0.02 -0.02\n",
      " -0.02 -0.01  0.   -0.02 -0.02 -0.01 -0.02 -0.03 -0.02  0.   -0.01 -0.01 -0.02 -0.03 -0.04 -0.05  0.1  -0.01 -0.01 -0.02 -0.03 -0.05  0.51]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RDDRRRRDDDD': 0}, pidx2states={0: [1, 2, 9, 16, 17, 18, 19, 20, 27, 34, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  30\n",
      "Reward Function:  [ 0.   -0.09 -0.04 -0.03 -0.02 -0.02 -0.01  0.   -0.04 -0.03 -0.02 -0.01 -0.01 -0.01  0.   -0.03 -0.02 -0.02 -0.02 -0.02 -0.01  0.   -0.02 -0.02 -0.02 -0.02\n",
      " -0.02 -0.02  0.   -0.02 -0.02 -0.01 -0.02 -0.03 -0.03  0.   -0.01 -0.01 -0.02 -0.03 -0.04 -0.06  0.1  -0.01 -0.01 -0.02 -0.03 -0.05  0.51]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRRRDR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 39, 40, 47, 48]})\n",
      "Cover:  {0: 0.003205128205128205, 1: 0.9967948717948718}\n",
      "Iteration:  31\n",
      "Reward Function:  [ 0.   -0.09 -0.04 -0.03 -0.02 -0.02 -0.01  0.   -0.05 -0.03 -0.02 -0.01 -0.01 -0.01  0.   -0.03 -0.02 -0.02 -0.02 -0.02 -0.01  0.   -0.02 -0.02 -0.02 -0.02\n",
      " -0.02 -0.02  0.   -0.02 -0.02 -0.01 -0.02 -0.03 -0.03  0.   -0.02 -0.02 -0.02 -0.03 -0.04 -0.06  0.1  -0.01 -0.01 -0.02 -0.03 -0.05  0.51]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'RRRRRDDDDDD': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 2, 3, 4, 5, 6, 13, 20, 27, 34, 41, 48]})\n",
      "Cover:  {0: 0.035256410256410256, 1: 0.9647435897435898}\n",
      "Iteration:  32\n",
      "Reward Function:  [ 0.01 -0.09 -0.05 -0.03 -0.02 -0.02 -0.01  0.01 -0.05 -0.03 -0.02 -0.01 -0.01 -0.01  0.01 -0.03 -0.02 -0.02 -0.02 -0.02 -0.02  0.01 -0.02 -0.02 -0.02 -0.02\n",
      " -0.02 -0.02  0.01 -0.02 -0.02 -0.01 -0.02 -0.03 -0.03  0.01 -0.02 -0.02 -0.02 -0.03 -0.04 -0.06  0.11 -0.01 -0.01 -0.02 -0.03 -0.05  0.5 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDRDRDDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 23, 30, 31, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.09294871794871795, 1: 0.907051282051282}\n",
      "Iteration:  33\n",
      "Reward Function:  [ 0.01 -0.09 -0.05 -0.03 -0.02 -0.02 -0.01  0.01 -0.05 -0.03 -0.02 -0.01 -0.01 -0.01  0.01 -0.03 -0.02 -0.02 -0.02 -0.02 -0.02  0.01 -0.03 -0.02 -0.02 -0.02\n",
      " -0.02 -0.02  0.01 -0.02 -0.02 -0.02 -0.02 -0.03 -0.03  0.01 -0.02 -0.02 -0.02 -0.03 -0.04 -0.06  0.11 -0.01 -0.01 -0.02 -0.03 -0.06  0.5 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'RRRDDDDRDDR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 2, 3, 4, 11, 18, 25, 32, 33, 40, 47, 48]})\n",
      "Cover:  {0: 0.14423076923076922, 1: 0.8557692307692307}\n",
      "Iteration:  34\n",
      "Reward Function:  [ 0.01 -0.09 -0.05 -0.04 -0.03 -0.02 -0.01  0.01 -0.05 -0.03 -0.02 -0.02 -0.01 -0.01  0.01 -0.03 -0.02 -0.02 -0.02 -0.02 -0.02  0.01 -0.03 -0.02 -0.02 -0.02\n",
      " -0.02 -0.02  0.01 -0.02 -0.02 -0.02 -0.02 -0.03 -0.03  0.01 -0.02 -0.02 -0.02 -0.03 -0.04 -0.06  0.11 -0.01 -0.01 -0.02 -0.03 -0.06  0.5 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'RDRRRDDDDDR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 2, 9, 10, 11, 12, 19, 26, 33, 40, 47, 48]})\n",
      "Cover:  {0: 0.20833333333333334, 1: 0.7916666666666666}\n",
      "Iteration:  35\n",
      "Reward Function:  [ 0.01 -0.09 -0.05 -0.04 -0.03 -0.02 -0.01  0.01 -0.05 -0.03 -0.03 -0.02 -0.02 -0.01  0.01 -0.03 -0.02 -0.02 -0.02 -0.02 -0.02  0.01 -0.03 -0.02 -0.02 -0.02\n",
      " -0.02 -0.02  0.01 -0.02 -0.02 -0.02 -0.02 -0.03 -0.03  0.01 -0.02 -0.02 -0.02 -0.03 -0.05 -0.06  0.12 -0.01 -0.01 -0.02 -0.03 -0.06  0.49]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDDRRRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 43, 44, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.2564102564102564, 1: 0.7435897435897436}\n",
      "Iteration:  36\n",
      "Reward Function:  [ 0.02 -0.09 -0.05 -0.04 -0.03 -0.02 -0.01  0.02 -0.05 -0.03 -0.03 -0.02 -0.02 -0.01  0.02 -0.04 -0.02 -0.02 -0.02 -0.02 -0.02  0.02 -0.03 -0.02 -0.02 -0.02\n",
      " -0.02 -0.02  0.02 -0.02 -0.02 -0.02 -0.02 -0.03 -0.03  0.02 -0.02 -0.02 -0.02 -0.03 -0.05 -0.06  0.12 -0.01 -0.02 -0.02 -0.03 -0.07  0.49]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'RRRRRDDDDDD': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 2, 3, 4, 5, 6, 13, 20, 27, 34, 41, 48]})\n",
      "Cover:  {0: 0.30128205128205127, 1: 0.6987179487179487}\n",
      "Iteration:  37\n",
      "Reward Function:  [ 0.02 -0.09 -0.06 -0.04 -0.03 -0.02 -0.01  0.02 -0.05 -0.03 -0.03 -0.02 -0.02 -0.01  0.02 -0.04 -0.02 -0.02 -0.02 -0.02 -0.02  0.02 -0.03 -0.02 -0.02 -0.02\n",
      " -0.02 -0.02  0.02 -0.02 -0.02 -0.02 -0.02 -0.03 -0.03  0.02 -0.02 -0.02 -0.02 -0.03 -0.05 -0.06  0.12 -0.01 -0.02 -0.02 -0.03 -0.07  0.48]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDRDRDRDRRD': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 16, 23, 24, 31, 32, 39, 40, 41, 48]})\n",
      "Cover:  {0: 0.33974358974358976, 1: 0.6602564102564102}\n",
      "Iteration:  38\n",
      "Reward Function:  [ 0.02 -0.09 -0.06 -0.04 -0.03 -0.02 -0.01  0.02 -0.06 -0.03 -0.03 -0.02 -0.02 -0.01  0.02 -0.04 -0.03 -0.02 -0.02 -0.02 -0.02  0.02 -0.03 -0.02 -0.02 -0.02\n",
      " -0.02 -0.02  0.02 -0.02 -0.02 -0.02 -0.03 -0.03 -0.03  0.02 -0.02 -0.02 -0.02 -0.03 -0.05 -0.07  0.12 -0.01 -0.02 -0.02 -0.03 -0.07  0.48]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'RRRRRDDDDDD': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 2, 3, 4, 5, 6, 13, 20, 27, 34, 41, 48]})\n",
      "Cover:  {0: 0.38782051282051283, 1: 0.6121794871794872}\n",
      "Iteration:  39\n",
      "Reward Function:  [ 0.03 -0.09 -0.06 -0.04 -0.03 -0.03 -0.01  0.03 -0.06 -0.03 -0.03 -0.02 -0.02 -0.02  0.03 -0.04 -0.03 -0.02 -0.02 -0.02 -0.02  0.03 -0.03 -0.02 -0.02 -0.02\n",
      " -0.02 -0.03  0.03 -0.02 -0.02 -0.02 -0.03 -0.03 -0.04  0.03 -0.02 -0.02 -0.02 -0.03 -0.05 -0.07  0.13 -0.01 -0.02 -0.02 -0.03 -0.07  0.48]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRRDRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 39, 46, 47, 48]})\n",
      "Cover:  {0: 0.4198717948717949, 1: 0.5801282051282052}\n",
      "Iteration:  40\n",
      "Reward Function:  [ 0.03 -0.09 -0.06 -0.04 -0.03 -0.03 -0.01  0.03 -0.06 -0.03 -0.03 -0.02 -0.02 -0.02  0.03 -0.04 -0.03 -0.02 -0.02 -0.02 -0.02  0.03 -0.03 -0.02 -0.02 -0.02\n",
      " -0.02 -0.03  0.03 -0.03 -0.02 -0.02 -0.03 -0.03 -0.04  0.03 -0.02 -0.02 -0.03 -0.04 -0.05 -0.07  0.13 -0.01 -0.02 -0.02 -0.04 -0.07  0.47]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRRRDR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 39, 40, 47, 48]})\n",
      "Cover:  {0: 0.4583333333333333, 1: 0.5416666666666666}\n",
      "Iteration:  41\n",
      "Reward Function:  [ 0.03 -0.09 -0.06 -0.04 -0.03 -0.03 -0.01  0.03 -0.06 -0.03 -0.03 -0.02 -0.02 -0.02  0.03 -0.04 -0.03 -0.02 -0.02 -0.02 -0.02  0.03 -0.04 -0.02 -0.02 -0.02\n",
      " -0.02 -0.03  0.03 -0.03 -0.02 -0.02 -0.03 -0.03 -0.04  0.03 -0.03 -0.02 -0.03 -0.04 -0.05 -0.07  0.13 -0.01 -0.02 -0.02 -0.04 -0.07  0.47]\n",
      "Finished BM Search. Entropy: 0.6896709283570267. Max Ent possible: 0.6931471805599453. Cover: {0: 0.4583333333333333, 1: 0.5416666666666666}. Behaviors: {0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 39, 40, 47, 48]}\n",
      "Behavior map:  ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRRRDR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 39, 40, 47, 48]})\n",
      "Reward Function:  [ 0.03 -0.09 -0.06 -0.04 -0.03 -0.03 -0.01  0.03 -0.06 -0.03 -0.03 -0.02 -0.02 -0.02  0.03 -0.04 -0.03 -0.02 -0.02 -0.02 -0.02  0.03 -0.03 -0.02 -0.02 -0.02\n",
      " -0.02 -0.03  0.03 -0.03 -0.02 -0.02 -0.03 -0.03 -0.04  0.03 -0.02 -0.02 -0.03 -0.04 -0.05 -0.07  0.13 -0.01 -0.02 -0.02 -0.04 -0.07  0.47]\n",
      "Finished episode 1.\n",
      "Started episode 2.\n",
      "Beginning calculation of log-likelihood. Calculating 400 samples.\n",
      "Mean Parameters: [0.639764829518934, 0.8566457787631506]\n",
      "Computed Region of Interest. Size = 0.61\n",
      "Initialized Reward Function R: [ 0.03 -0.09 -0.06 -0.04 -0.03 -0.03 -0.01  0.03 -0.06 -0.03 -0.03 -0.02 -0.02 -0.02  0.03 -0.04 -0.03 -0.02 -0.02 -0.02 -0.02  0.03 -0.03 -0.02 -0.02 -0.02\n",
      " -0.02 -0.03  0.03 -0.03 -0.02 -0.02 -0.03 -0.03 -0.04  0.03 -0.02 -0.02 -0.03 -0.04 -0.05 -0.07  0.13 -0.01 -0.02 -0.02 -0.04 -0.07  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RDDDRRDRDRD': 0}, pidx2states={0: [1, 2, 9, 16, 23, 24, 25, 32, 33, 40, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  1\n",
      "Reward Function:  [ 0.03 -0.09 -0.06 -0.04 -0.03 -0.03 -0.01  0.03 -0.06 -0.04 -0.03 -0.02 -0.02 -0.02  0.03 -0.04 -0.03 -0.02 -0.02 -0.02 -0.02  0.03 -0.03 -0.03 -0.02 -0.02\n",
      " -0.02 -0.03  0.03 -0.03 -0.02 -0.02 -0.03 -0.04 -0.04  0.03 -0.02 -0.02 -0.03 -0.04 -0.05 -0.07  0.13 -0.01 -0.02 -0.02 -0.04 -0.07  0.54]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRRDRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 39, 46, 47, 48]})\n",
      "Cover:  {0: 0.2757201646090535, 1: 0.7242798353909465}\n",
      "Iteration:  2\n",
      "Reward Function:  [ 0.03 -0.09 -0.06 -0.04 -0.03 -0.03 -0.01  0.03 -0.06 -0.04 -0.03 -0.02 -0.02 -0.02  0.03 -0.04 -0.03 -0.02 -0.02 -0.02 -0.02  0.03 -0.04 -0.03 -0.02 -0.02\n",
      " -0.02 -0.03  0.03 -0.03 -0.02 -0.02 -0.03 -0.04 -0.04  0.03 -0.03 -0.02 -0.03 -0.04 -0.05 -0.07  0.13 -0.01 -0.02 -0.02 -0.04 -0.07  0.54]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'RRRDDDRDDRD': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 2, 3, 4, 11, 18, 25, 26, 33, 40, 41, 48]})\n",
      "Cover:  {0: 0.32098765432098764, 1: 0.6790123456790124}\n",
      "Iteration:  3\n",
      "Reward Function:  [ 0.04 -0.09 -0.06 -0.04 -0.04 -0.03 -0.01  0.04 -0.06 -0.04 -0.03 -0.02 -0.02 -0.02  0.04 -0.04 -0.03 -0.02 -0.02 -0.02 -0.02  0.04 -0.04 -0.03 -0.02 -0.03\n",
      " -0.03 -0.03  0.04 -0.03 -0.02 -0.02 -0.03 -0.04 -0.04  0.04 -0.03 -0.02 -0.03 -0.04 -0.06 -0.08  0.14 -0.01 -0.02 -0.02 -0.04 -0.07  0.53]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDRRRDRDR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 30, 31, 32, 39, 40, 47, 48]})\n",
      "Cover:  {0: 0.3662551440329218, 1: 0.6337448559670782}\n",
      "Iteration:  4\n",
      "Reward Function:  [ 0.04 -0.09 -0.06 -0.04 -0.04 -0.03 -0.01  0.04 -0.07 -0.04 -0.03 -0.02 -0.02 -0.02  0.04 -0.05 -0.03 -0.02 -0.02 -0.02 -0.02  0.04 -0.04 -0.03 -0.02 -0.03\n",
      " -0.03 -0.03  0.04 -0.03 -0.02 -0.02 -0.03 -0.04 -0.04  0.04 -0.03 -0.02 -0.03 -0.04 -0.06 -0.08  0.14 -0.01 -0.02 -0.02 -0.04 -0.08  0.53]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'RRRRDDDDRDD': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 2, 3, 4, 5, 12, 19, 26, 33, 34, 41, 48]})\n",
      "Cover:  {0: 0.411522633744856, 1: 0.588477366255144}\n",
      "Iteration:  5\n",
      "Reward Function:  [ 0.04 -0.09 -0.07 -0.05 -0.04 -0.03 -0.01  0.04 -0.07 -0.04 -0.03 -0.02 -0.02 -0.02  0.04 -0.05 -0.03 -0.02 -0.02 -0.03 -0.02  0.04 -0.04 -0.03 -0.02 -0.03\n",
      " -0.03 -0.03  0.04 -0.03 -0.02 -0.02 -0.03 -0.04 -0.04  0.04 -0.03 -0.02 -0.03 -0.04 -0.06 -0.08  0.15 -0.01 -0.02 -0.02 -0.04 -0.08  0.52]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "       1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDDRRRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 43, 44, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.448559670781893, 1: 0.551440329218107}\n",
      "Iteration:  6\n",
      "Reward Function:  [ 0.05 -0.09 -0.07 -0.05 -0.04 -0.03 -0.01  0.05 -0.07 -0.04 -0.03 -0.02 -0.02 -0.02  0.05 -0.05 -0.03 -0.02 -0.02 -0.03 -0.02  0.05 -0.04 -0.03 -0.02 -0.03\n",
      " -0.03 -0.03  0.05 -0.04 -0.02 -0.02 -0.03 -0.04 -0.04  0.05 -0.03 -0.02 -0.03 -0.04 -0.06 -0.08  0.15 -0.02 -0.02 -0.03 -0.04 -0.08  0.52]\n",
      "Finished BM Search. Entropy: 0.6878455900220348. Max Ent possible: 0.6931471805599453. Cover: {0: 0.448559670781893, 1: 0.551440329218107}. Behaviors: {0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 43, 44, 45, 46, 47, 48]}\n",
      "Behavior map:  ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "       1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDDRRRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 43, 44, 45, 46, 47, 48]})\n",
      "Reward Function:  [ 0.04 -0.09 -0.07 -0.05 -0.04 -0.03 -0.01  0.04 -0.07 -0.04 -0.03 -0.02 -0.02 -0.02  0.04 -0.05 -0.03 -0.02 -0.02 -0.03 -0.02  0.04 -0.04 -0.03 -0.02 -0.03\n",
      " -0.03 -0.03  0.04 -0.03 -0.02 -0.02 -0.03 -0.04 -0.04  0.04 -0.03 -0.02 -0.03 -0.04 -0.06 -0.08  0.15 -0.01 -0.02 -0.02 -0.04 -0.08  0.52]\n",
      "Finished episode 2.\n",
      "Started episode 3.\n",
      "Beginning calculation of log-likelihood. Calculating 400 samples.\n",
      "Mean Parameters: [0.6424273213670059, 0.836154142627434]\n",
      "Computed Region of Interest. Size = 0.25\n",
      "Initialized Reward Function R: [ 0.04 -0.09 -0.07 -0.05 -0.04 -0.03 -0.01  0.04 -0.07 -0.04 -0.03 -0.02 -0.02 -0.02  0.04 -0.05 -0.03 -0.02 -0.02 -0.03 -0.02  0.04 -0.04 -0.03 -0.02 -0.03\n",
      " -0.03 -0.03  0.04 -0.03 -0.02 -0.02 -0.03 -0.04 -0.04  0.04 -0.03 -0.02 -0.03 -0.04 -0.06 -0.08  0.15 -0.01 -0.02 -0.02 -0.04 -0.08  0.58]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32), p2idx={'RDDDRRDRDRD': 0}, pidx2states={0: [1, 2, 9, 16, 23, 24, 25, 32, 33, 40, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  1\n",
      "Reward Function:  [ 0.04 -0.09 -0.07 -0.05 -0.04 -0.03 -0.01  0.04 -0.07 -0.04 -0.03 -0.02 -0.02 -0.02  0.04 -0.05 -0.03 -0.02 -0.02 -0.03 -0.02  0.04 -0.04 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.04 -0.03 -0.02 -0.02 -0.04 -0.05 -0.04  0.04 -0.03 -0.02 -0.03 -0.04 -0.07 -0.08  0.15 -0.01 -0.02 -0.02 -0.04 -0.08  0.58]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
      "      dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRRDRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 39, 46, 47, 48]})\n",
      "Cover:  {0: 0.9405940594059405, 1: 0.0594059405940594}\n",
      "Iteration:  2\n",
      "Reward Function:  [ 0.04 -0.09 -0.07 -0.05 -0.04 -0.03 -0.01  0.04 -0.06 -0.04 -0.03 -0.02 -0.02 -0.02  0.04 -0.04 -0.03 -0.02 -0.02 -0.03 -0.02  0.04 -0.04 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.04 -0.03 -0.02 -0.02 -0.04 -0.05 -0.04  0.04 -0.02 -0.02 -0.03 -0.04 -0.07 -0.08  0.14 -0.01 -0.02 -0.02 -0.04 -0.07  0.58]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
      "      dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRRDRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 39, 46, 47, 48]})\n",
      "Cover:  {0: 0.900990099009901, 1: 0.09900990099009901}\n",
      "Iteration:  3\n",
      "Reward Function:  [ 0.04 -0.09 -0.07 -0.05 -0.04 -0.03 -0.01  0.04 -0.06 -0.04 -0.03 -0.02 -0.02 -0.02  0.04 -0.04 -0.03 -0.02 -0.02 -0.03 -0.02  0.04 -0.03 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.04 -0.03 -0.02 -0.02 -0.04 -0.05 -0.04  0.04 -0.02 -0.02 -0.02 -0.04 -0.07 -0.08  0.14 -0.01 -0.02 -0.02 -0.03 -0.07  0.59]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1],\n",
      "      dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRRDRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 39, 46, 47, 48]})\n",
      "Cover:  {0: 0.8316831683168316, 1: 0.16831683168316833}\n",
      "Iteration:  4\n",
      "Reward Function:  [ 0.03 -0.09 -0.07 -0.05 -0.04 -0.03 -0.01  0.03 -0.06 -0.04 -0.03 -0.02 -0.02 -0.02  0.03 -0.04 -0.03 -0.02 -0.02 -0.03 -0.02  0.03 -0.03 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.02 -0.01 -0.02 -0.03 -0.07 -0.08  0.14 -0.01 -0.02 -0.02 -0.03 -0.07  0.59]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1],\n",
      "      dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7524752475247525, 1: 0.24752475247524752}\n",
      "Iteration:  5\n",
      "Reward Function:  [ 0.03 -0.09 -0.07 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.04 -0.03 -0.02 -0.02 -0.02  0.03 -0.04 -0.03 -0.02 -0.02 -0.03 -0.02  0.03 -0.03 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.02 -0.03 -0.07 -0.08  0.13 -0.01 -0.02 -0.02 -0.03 -0.06  0.59]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1],\n",
      "      dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6138613861386139, 1: 0.38613861386138615}\n",
      "Iteration:  6\n",
      "Reward Function:  [ 0.03 -0.09 -0.07 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.04 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.03 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.08  0.13 -0.01 -0.02 -0.02 -0.02 -0.06  0.6 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "       1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1],\n",
      "      dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.504950495049505, 1: 0.49504950495049505}\n",
      "Iteration:  7\n",
      "Reward Function:  [ 0.03 -0.09 -0.07 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.04 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.03 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.03 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.08  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.6 ]\n",
      "Finished BM Search. Entropy: 0.6930981649566266. Max Ent possible: 0.6931471805599453. Cover: {0: 0.504950495049505, 1: 0.49504950495049505}. Behaviors: {0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]}\n",
      "Behavior map:  ExperimentResult(data=array([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "       1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1],\n",
      "      dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Reward Function:  [ 0.03 -0.09 -0.07 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.04 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.03 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.08  0.13 -0.01 -0.02 -0.02 -0.02 -0.06  0.6 ]\n",
      "Finished episode 3.\n",
      "Started episode 4.\n",
      "Beginning calculation of log-likelihood. Calculating 400 samples.\n",
      "Mean Parameters: [0.6564191941907833, 0.8197011128791836]\n",
      "Computed Region of Interest. Size = 0.16\n",
      "Initialized Reward Function R: [ 0.03 -0.09 -0.07 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.04 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.03 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.08  0.13 -0.01 -0.02 -0.02 -0.02 -0.06  0.66]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RDDDRRDRDRD': 0}, pidx2states={0: [1, 2, 9, 16, 23, 24, 25, 32, 33, 40, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  1\n",
      "Reward Function:  [ 0.03 -0.1  -0.07 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.04 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.02 -0.02 -0.06  0.65]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "       1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  2\n",
      "Reward Function:  [ 0.03 -0.1  -0.07 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.04 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.66]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.5079365079365079, 1: 0.49206349206349204}\n",
      "Iteration:  3\n",
      "Reward Function:  [ 0.02 -0.1  -0.07 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.04 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.66]\n",
      "Finished BM Search. Entropy: 0.6930211989529488. Max Ent possible: 0.6931471805599453. Cover: {0: 0.5079365079365079, 1: 0.49206349206349204}. Behaviors: {0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]}\n",
      "Behavior map:  ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Reward Function:  [ 0.03 -0.1  -0.07 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.04 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.66]\n",
      "Finished episode 4.\n",
      "Started episode 5.\n",
      "Beginning calculation of log-likelihood. Calculating 400 samples.\n",
      "Mean Parameters: [0.6960953580676982, 0.8069632702509709]\n",
      "Computed Region of Interest. Size = 0.09\n",
      "Initialized Reward Function R: [ 0.03 -0.1  -0.07 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.04 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.03 -0.03 -0.03\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RDDDRRDRDRD': 0}, pidx2states={0: [1, 2, 9, 16, 23, 24, 25, 32, 33, 40, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  1\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  2\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  3\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  4\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  5\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  6\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  7\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  8\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  9\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  10\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  11\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  12\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  13\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  14\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  15\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  16\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  17\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  18\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  19\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  20\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  21\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  22\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  23\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  24\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  25\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  26\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  27\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  28\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  29\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  30\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  31\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  32\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  33\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  34\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  35\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  36\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  37\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  38\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  39\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  40\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  41\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  42\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  43\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  44\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  45\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  46\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  47\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  48\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  49\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  50\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  51\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  52\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  53\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  54\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  55\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  56\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  57\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  58\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  59\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  60\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  61\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  62\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  63\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  64\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  65\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  66\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  67\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  68\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  69\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  70\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  71\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  72\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  73\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  74\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  75\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  76\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  77\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  78\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  79\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  80\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  81\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  82\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  83\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7142857142857143, 1: 0.2857142857142857}\n",
      "Iteration:  84\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  85\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6857142857142857, 1: 0.3142857142857143}\n",
      "Iteration:  86\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  87\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6857142857142857, 1: 0.3142857142857143}\n",
      "Iteration:  88\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  89\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6857142857142857, 1: 0.3142857142857143}\n",
      "Iteration:  90\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  91\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6857142857142857, 1: 0.3142857142857143}\n",
      "Iteration:  92\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  93\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6857142857142857, 1: 0.3142857142857143}\n",
      "Iteration:  94\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  95\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6857142857142857, 1: 0.3142857142857143}\n",
      "Iteration:  96\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  97\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6857142857142857, 1: 0.3142857142857143}\n",
      "Iteration:  98\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "Iteration:  99\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.06  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6857142857142857, 1: 0.3142857142857143}\n",
      "Iteration:  100\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.37142857142857144, 1: 0.6285714285714286}\n",
      "\n",
      "\n",
      "Reached Maximum Number of BM Computations. Terminating BM Search.\n",
      "\n",
      "\n",
      "Finished BM Search. Entropy: 0.659711615178473. Max Ent possible: 0.6931471805599453. Cover: {0: 0.37142857142857144, 1: 0.6285714285714286}. Behaviors: {0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]}\n",
      "Behavior map:  ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Finished episode 5.\n",
      "Started episode 6.\n",
      "Beginning calculation of log-likelihood. Calculating 400 samples.\n",
      "Mean Parameters: [0.7112898585688657, 0.8020336160095882]\n",
      "Computed Region of Interest. Size = 0.06\n",
      "Initialized Reward Function R: [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'RDDDRRDRDRD': 0}, pidx2states={0: [1, 2, 9, 16, 23, 24, 25, 32, 33, 40, 41, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  1\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.4583333333333333, 1: 0.5416666666666666}\n",
      "Iteration:  2\n",
      "Reward Function:  [ 0.03 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.03 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.03 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.03 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.03 -0.02 -0.02 -0.02 -0.04 -0.05 -0.04  0.03 -0.01 -0.01 -0.01 -0.03 -0.07 -0.09  0.13 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Finished BM Search. Entropy: 0.6896709283570267. Max Ent possible: 0.6931471805599453. Cover: {0: 0.4583333333333333, 1: 0.5416666666666666}. Behaviors: {0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]}\n",
      "Behavior map:  ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.7 ]\n",
      "Finished episode 6.\n",
      "Started episode 7.\n",
      "Beginning calculation of log-likelihood. Calculating 400 samples.\n",
      "Mean Parameters: [0.7579882622262983, 0.7974472779751985]\n",
      "Computed Region of Interest. Size = 0.04\n",
      "Initialized Reward Function R: [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.03 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.02 -0.04 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.03 -0.07 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DRDDRDRDRDR': 0}, pidx2states={0: [1, 8, 9, 16, 23, 24, 31, 32, 39, 40, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  1\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  2\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  3\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  4\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  5\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  6\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  7\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  8\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  9\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  10\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  11\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  12\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  13\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  14\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  15\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  16\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  17\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  18\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  19\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  20\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  21\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  22\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  23\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  24\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  25\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  26\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  27\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  28\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  29\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  30\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  31\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  32\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  33\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  34\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  35\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  36\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  37\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  38\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  39\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  40\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  41\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  42\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  43\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  44\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  45\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  46\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  47\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  48\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  49\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  50\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  51\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  52\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  53\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  54\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  55\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  56\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  57\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  58\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  59\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  60\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  61\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  62\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  63\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  64\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  65\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  66\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  67\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  68\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  69\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  70\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  71\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  72\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  73\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  74\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  75\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  76\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  77\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  78\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  79\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  80\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  81\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  82\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  83\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  84\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  85\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  86\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  87\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  88\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  89\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  90\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  91\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  92\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  93\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  94\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  95\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  96\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  97\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  98\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "Iteration:  99\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.6470588235294118, 1: 0.35294117647058826}\n",
      "Iteration:  100\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.04 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Behavior Map: ExperimentResult(data=array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.058823529411764705, 1: 0.9411764705882353}\n",
      "\n",
      "\n",
      "Reached Maximum Number of BM Computations. Terminating BM Search.\n",
      "\n",
      "\n",
      "Finished BM Search. Entropy: 0.649248354870898. Max Ent possible: 0.6931471805599453. Cover: {0: 0.6470588235294118, 1: 0.35294117647058826}. Behaviors: {0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]}\n",
      "Behavior map:  ExperimentResult(data=array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Reward Function:  [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.69]\n",
      "Finished episode 7.\n",
      "Started episode 8.\n",
      "Beginning calculation of log-likelihood. Calculating 400 samples.\n",
      "Mean Parameters: [0.7692025125075798, 0.7950163123223427]\n",
      "Computed Region of Interest. Size = 0.03\n",
      "Initialized Reward Function R: [ 0.02 -0.1  -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.04 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DRDDRDRDRDR': 0}, pidx2states={0: [1, 8, 9, 16, 23, 24, 31, 32, 39, 40, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  1\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'LDDDDDD': 0}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  2\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  3\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  4\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  5\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  6\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  7\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  8\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  9\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  10\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  11\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  12\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  13\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  14\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  15\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  16\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  17\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  18\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  19\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  20\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  21\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  22\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  23\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  24\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  25\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  26\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  27\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  28\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  29\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  30\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  31\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  32\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  33\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  34\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  35\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  36\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  37\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  38\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  39\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  40\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  41\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  42\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  43\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  44\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  45\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  46\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  47\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  48\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  49\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  50\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  51\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  52\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  53\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  54\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  55\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  56\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  57\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  58\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  59\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  60\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  61\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  62\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  63\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  64\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  65\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  66\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  67\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  68\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  69\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  70\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  71\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  72\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  73\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  74\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  75\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  76\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  77\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  78\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  79\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  80\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  81\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  82\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  83\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  84\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  85\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  86\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  87\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  88\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  89\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  90\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  91\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  92\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  93\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  94\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  95\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  96\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  97\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.05  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  98\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "Iteration:  99\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.02 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.63]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.25, 1: 0.75}\n",
      "Iteration:  100\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.75, 1: 0.25}\n",
      "\n",
      "\n",
      "Reached Maximum Number of BM Computations. Terminating BM Search.\n",
      "\n",
      "\n",
      "Finished BM Search. Entropy: 0.5623351446188083. Max Ent possible: 0.6931471805599453. Cover: {0: 0.75, 1: 0.25}. Behaviors: {0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]}\n",
      "Behavior map:  ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.62]\n",
      "Finished episode 8.\n",
      "Started episode 9.\n",
      "Beginning calculation of log-likelihood. Calculating 400 samples.\n",
      "Mean Parameters: [0.8088177320771474, 0.7913617882320141]\n",
      "Computed Region of Interest. Size = 0.02\n",
      "Initialized Reward Function R: [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'DDRDRDRDRDR': 0}, pidx2states={0: [1, 8, 15, 16, 23, 24, 31, 32, 39, 40, 47, 48]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  1\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), p2idx={'LDDDDDD': 0}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42]})\n",
      "Cover:  {0: 1.0}\n",
      "Iteration:  2\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7777777777777778, 1: 0.2222222222222222}\n",
      "Iteration:  3\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 1, 1, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.3333333333333333, 1: 0.6666666666666666}\n",
      "Iteration:  4\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7777777777777778, 1: 0.2222222222222222}\n",
      "Iteration:  5\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 1, 1, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.3333333333333333, 1: 0.6666666666666666}\n",
      "Iteration:  6\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7777777777777778, 1: 0.2222222222222222}\n",
      "Iteration:  7\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 1, 1, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.3333333333333333, 1: 0.6666666666666666}\n",
      "Iteration:  8\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 0, 0, 0, 0, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.7777777777777778, 1: 0.2222222222222222}\n",
      "Iteration:  9\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.   -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.55]\n",
      "Behavior Map: ExperimentResult(data=array([0, 0, 1, 1, 1, 1, 1, 0, 1], dtype=int32), p2idx={'LDDDDDD': 0, 'DDDDDRRDRRR': 1}, pidx2states={0: [1, 0, 7, 14, 21, 28, 35, 42], 1: [1, 8, 15, 22, 29, 36, 37, 38, 45, 46, 47, 48]})\n",
      "Cover:  {0: 0.3333333333333333, 1: 0.6666666666666666}\n",
      "Iteration:  10\n",
      "Reward Function:  [ 0.02 -0.11 -0.08 -0.05 -0.04 -0.03 -0.01  0.02 -0.05 -0.05 -0.03 -0.02 -0.02 -0.02  0.02 -0.03 -0.05 -0.02 -0.02 -0.03 -0.02  0.02 -0.02 -0.04 -0.04 -0.04\n",
      " -0.03 -0.03  0.02 -0.01 -0.02 -0.03 -0.05 -0.05 -0.04  0.02 -0.01 -0.   -0.01 -0.04 -0.08 -0.09  0.12 -0.01 -0.02 -0.01 -0.02 -0.06  0.55]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m candidate_environments_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_iterations_gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m candidate_environments_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstepsize_gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0005\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43menv_design\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_n_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcandidate_environments_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidate_environments_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/classes/CS282R/irl-environment-design/src/utils/environment_design.py:254\u001b[0m, in \u001b[0;36mEnvironmentDesign.run_n_episodes\u001b[0;34m(self, n_episodes, candidate_environments_args, bayesian_regret_how, verbose)\u001b[0m\n\u001b[1;32m    241\u001b[0m entropy_bm \u001b[38;5;241m=\u001b[39m EntropyBM(estimate_R \u001b[38;5;241m=\u001b[39m estimate_R,\n\u001b[1;32m    242\u001b[0m                        estimate_T \u001b[38;5;241m=\u001b[39m estimate_T,\n\u001b[1;32m    243\u001b[0m                        estimate_gamma \u001b[38;5;241m=\u001b[39m estimate_gamma,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m                        verbose\u001b[38;5;241m=\u001b[39mverbose\n\u001b[1;32m    249\u001b[0m                        )\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m#Find a reward function that maximizes the entropy of the Behavior Map. \u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m#TODO: also use transition function. Currently only do gradient updates on R. Do we want this?\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m updated_reward \u001b[38;5;241m=\u001b[39m \u001b[43mentropy_bm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBM_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_environment\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_environment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mnamed_parameter_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_named_parameter_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mshaped_parameter_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshaped_parameter_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mn_compute_BM\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcandidate_environments_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_compute_BM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mn_iterations_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidate_environments_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_iterations_gradient\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mstepsize_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidate_environments_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstepsize_gradient\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m#Generate an environment in which we observe the human with maximal information gain.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m optimal_environment \u001b[38;5;241m=\u001b[39m deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_environment)\n",
      "File \u001b[0;32m~/Desktop/Uni/classes/CS282R/irl-environment-design/src/utils/make_candidate_environments.py:196\u001b[0m, in \u001b[0;36mEntropyBM.BM_search\u001b[0;34m(self, base_environment, named_parameter_mesh, shaped_parameter_mesh, n_compute_BM, n_iterations_gradient, stepsize_gradient)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# for _ in range(n_compute_BM):\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m entropy_maximized:\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Compute Behavior Map\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     bm_out \u001b[38;5;241m=\u001b[39m \u001b[43mbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_behavior_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mreward_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mR_entropy_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mparameter_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamed_parameter_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mshaped_parameter_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshaped_parameter_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mregion_of_interest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mregion_of_interest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBehavior Map:\u001b[39m\u001b[38;5;124m\"\u001b[39m, bm_out)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m#Compute entropy of BM\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Uni/classes/CS282R/irl-environment-design/src/utils/behavior_map.py:55\u001b[0m, in \u001b[0;36mcalculate_behavior_map\u001b[0;34m(environment, reward_update, parameter_mesh, shaped_parameter_mesh, region_of_interest)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx_parameter, parameter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(parameter_mesh):\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m#Compute BM restricted to ROI.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx_parameter \u001b[38;5;129;01min\u001b[39;00m region_of_interest:\n\u001b[1;32m     53\u001b[0m     \n\u001b[1;32m     54\u001b[0m         \u001b[38;5;66;03m#Get the transition function, reward function, and gamma from the parameter.\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m         _transition_func \u001b[38;5;241m=\u001b[39m \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparameter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m         _reward_func \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mreward_function(\u001b[38;5;241m*\u001b[39mparameter\u001b[38;5;241m.\u001b[39mR)\n\u001b[1;32m     57\u001b[0m         _gamma \u001b[38;5;241m=\u001b[39m parameter\u001b[38;5;241m.\u001b[39mgamma\n",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m, in \u001b[0;36mcustom_transition_func\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_transition_func\u001b[39m(p):\n\u001b[0;32m---> 29\u001b[0m     _T \u001b[38;5;241m=\u001b[39m \u001b[43mtransition_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabsorbing_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgoal_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     _T \u001b[38;5;241m=\u001b[39m insert_walls_into_T(T\u001b[38;5;241m=\u001b[39m_T, wall_indices\u001b[38;5;241m=\u001b[39mwall_states)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _T\n",
      "File \u001b[0;32m~/Desktop/Uni/classes/CS282R/irl-environment-design/src/utils/make_environment.py:46\u001b[0m, in \u001b[0;36mtransition_matrix\u001b[0;34m(N, M, p, absorbing_states)\u001b[0m\n\u001b[1;32m     43\u001b[0m             T[s, a, neighbors[action]] \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m other_action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m {action}:\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;66;03m# for other_action in set([\"N\", \"E\", \"S\", \"W\"]) - {action}:\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m                 T[s, a, neighbors[other_action]] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Make the transition matrix absorbing\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# make_absorbing(absorbing_states, T)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m T\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "candidate_environments_args = {}\n",
    "candidate_environments_args[\"generate_how\"] = \"entropy_BM\"\n",
    "candidate_environments_args[\"n_compute_BM\"] = 100\n",
    "candidate_environments_args[\"n_iterations_gradient\"] = 1\n",
    "candidate_environments_args[\"stepsize_gradient\"] = 0.0005\n",
    "\n",
    "\n",
    "env_design.run_n_episodes(n_episodes = 25,\n",
    "                          candidate_environments_args=candidate_environments_args,\n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x718ed8d0b820>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = env_design.diagnostics[\"parameter_means\"]\n",
    "params = np.array(params)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "roi_sizes = env_design.diagnostics[\"ROI_sizes\"]\n",
    "roi_sizes = np.array(roi_sizes)\n",
    "axs[0].plot(roi_sizes, \"-x\", label=\"ROI Size\")\n",
    "axs[0].set_xlabel(\"Episode\")\n",
    "axs[0].set_ylabel(\"Size of ROI\")\n",
    "axs[0].set_ylim(0,1)\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"Size of ROI over time when learning R and $\\gamma$\")\n",
    "\n",
    "axs[1].plot(params[:, 0], color=\"orange\", label=\"Estimated gamma\")\n",
    "axs[1].plot(params[:, 1], color=\"blue\", label=\"Estimated p\")\n",
    "axs[1].hlines(true_params.R, 0, params.shape[0], color=\"orange\", linestyle=\"--\", label=\"True Gamma\")\n",
    "axs[1].hlines(true_params.gamma, 0, params.shape[0], color=\"blue\", linestyle=\"--\", label=\"True p\")\n",
    "axs[1].set_ylim(0,1)\n",
    "axs[1].set_title(\"Parameter Estimates over Time, Cliff World\")\n",
    "axs[1].set_xlabel(\"Episodes\")\n",
    "axs[1].set_ylabel(\"Parameter Value\")\n",
    "axs[1].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
