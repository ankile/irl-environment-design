{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr_dir = os.path.split(os.getcwd())[1]\n",
    "if curr_dir != \"irl-environment-design\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.utils.constants import ParamTuple\n",
    "from src.utils.make_environment import (\n",
    "    transition_matrix,\n",
    "    Environment,\n",
    "    insert_walls_into_T,\n",
    ")\n",
    "\n",
    "height = 7\n",
    "width = 7\n",
    "rewards = np.zeros((height, width))\n",
    "wall_states = np.zeros((height, width))\n",
    "\n",
    "rewards[6,0] = 1\n",
    "rewards[6,6] = 3\n",
    "\n",
    "# rewards[5,0] = -1\n",
    "# rewards[4,0] = -1\n",
    "\n",
    "rewards = rewards.flatten()\n",
    "\n",
    "goal_states = np.where(rewards > 0)[0]\n",
    "\n",
    "wall_states[2,0] = 1\n",
    "# wall_states[4,1] = 1\n",
    "\n",
    "wall_states = wall_states.flatten()\n",
    "wall_states = np.where(wall_states > 0)[0]\n",
    "\n",
    "agent_p = 0.9\n",
    "agent_gamma = 0.7\n",
    "p_true=1\n",
    "\n",
    "true_params = ParamTuple(agent_p, agent_gamma, rewards)\n",
    "\n",
    "wall_states = [14]\n",
    "\n",
    "T_true = transition_matrix(height, width, p=p_true, absorbing_states=goal_states)\n",
    "T_True = insert_walls_into_T(T=T_true, wall_indices=wall_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration to find fixed point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def soft_q_iteration_torch(\n",
    "    R: torch.Tensor,  # R is a one-dimensional tensor with shape (n_states,)\n",
    "    T_agent: torch.Tensor,\n",
    "    gamma: float,\n",
    "    beta: float,  # Inverse temperature parameter for the softmax function\n",
    "    tol: float = 1e-6,\n",
    ") -> torch.Tensor:\n",
    "    n_states, n_actions, _ = T_agent.shape\n",
    "    V = torch.zeros(n_states)\n",
    "    Q = torch.zeros((n_states, n_actions))\n",
    "    policy = torch.zeros((n_states, n_actions))\n",
    "\n",
    "    while True:\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                # Calculate the Q-value for action a in state s\n",
    "                Q[s, a] = R[s] + gamma * torch.dot(T_agent[s, a], V)\n",
    "\n",
    "        # Apply softmax to get a probabilistic policy\n",
    "        # max_Q = torch.max(Q, axis=1, keepdim=True)[0]\n",
    "        exp_Q = torch.exp(beta * (Q))  # Subtract max_Q for numerical stability\n",
    "        # exp_Q = torch.exp(beta * (Q - max_Q))  # Subtract max_Q for numerical stability\n",
    "\n",
    "        policy = exp_Q / torch.sum(exp_Q, axis=1, keepdim=True)\n",
    "\n",
    "        # Calculate the value function V using the probabilistic policy\n",
    "        V_new = torch.sum(policy * Q, axis=1)\n",
    "\n",
    "        # Check for convergence\n",
    "        if torch.max(torch.abs(V - V_new)) < tol:\n",
    "            break\n",
    "\n",
    "        V = V_new\n",
    "\n",
    "    return Q, V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_star, V_star, policy_star = soft_q_iteration_torch(R = torch.tensor(rewards, dtype=torch.float32), T_agent = torch.tensor(T_True, dtype=torch.float32), gamma = agent_gamma, beta = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Bellman Operator for fixed point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = torch.tensor(rewards, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_bellman_update_V(R, gamma, T, V):\n",
    "    return torch.log(torch.sum(torch.exp(torch.matmul(T, R+gamma*V)), axis=1))\n",
    "\n",
    "def soft_bellman_FP_V(R, gamma, T, V):\n",
    "    return soft_bellman_update_V(R, gamma, T, V) - V\n",
    "\n",
    "def soft_V_iteration_torch(R, gamma, T, V_init=None, tol=1e-6):\n",
    "    if V_init is None:\n",
    "        V_init = torch.zeros(49)\n",
    "\n",
    "    V = V_init\n",
    "    \n",
    "    while True:\n",
    "        V_new = soft_bellman_update_V(R, gamma,T, V)\n",
    "        if torch.max(torch.abs(V - V_new)) < tol:\n",
    "            break\n",
    "        V = V_new\n",
    "    return V\n",
    "\n",
    "V_star_parallel = soft_V_iteration_torch(R, agent_gamma, torch.tensor(T_True, dtype=torch.float32), V_init=None, tol=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_bellman_update(R, gamma, T, Q):\n",
    "    return R + gamma * torch.matmul(T, torch.log(torch.sum(torch.exp(Q), axis=1)))\n",
    "\n",
    "def soft_bellman_fp(R, gamma, T, Q):\n",
    "    return soft_bellman_update(R, gamma, T, Q) - Q\n",
    "\n",
    "def soft_q_iteration_torch(R, gamma, T, Q_init=None, tol=1e-6):\n",
    "    if Q_init is None:\n",
    "        Q_init = torch.zeros(R.shape[0], 4)\n",
    "\n",
    "    Q = Q_init\n",
    "    \n",
    "    while True:\n",
    "        Q_new = soft_bellman_update(R, gamma,T, Q)\n",
    "        if torch.max(torch.abs(Q - Q_new)) < tol:\n",
    "            break\n",
    "        Q = Q_new\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = torch.tensor(rewards, dtype=torch.float32, requires_grad=True)\n",
    "R_stretched = R.unsqueeze(1).repeat(1,4) # Stretch R to match the shape of Q, e.g. we switch from R(s) to R(s,a).\n",
    "\n",
    "Q_star = soft_q_iteration_torch(R_stretched, agent_gamma, torch.tensor(T_True, dtype=torch.float32), Q_init=None, tol=1e-6)\n",
    "\n",
    "beta = 1\n",
    "exp_Q = torch.exp(beta * (Q_star))  # Subtract max_Q for numerical stability\n",
    "\n",
    "policy = exp_Q / torch.sum(exp_Q, axis=1, keepdim=True)\n",
    "\n",
    "# Calculate the value function V using the probabilistic policy\n",
    "V_new = torch.sum(policy * Q_star, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Derivative of Value Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_grad = torch.autograd.Variable(R, requires_grad=True)\n",
    "T_torch = torch.tensor(T_True, dtype=torch.float32)\n",
    "T_grad = torch.autograd.Variable(T_torch, requires_grad=True)\n",
    "V_star_grad = torch.autograd.Variable(V_star_parallel, requires_grad=True)\n",
    "\n",
    "psi = soft_bellman_FP_V(R = R_grad, gamma=agent_gamma, T=T_grad, V = V_star_grad)\n",
    "\n",
    "psi.backward(torch.ones_like(psi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian = torch.autograd.functional.jacobian(func=soft_bellman_FP_V, inputs=(R_grad, torch.tensor(agent_gamma, dtype = torch.float32), T_grad, V_star_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.4597, 3.4324, 3.3718, 3.3177, 3.2785, 3.2565, 3.2535, 2.9744, 3.3743,\n",
       "        3.3582, 3.2967, 3.2583, 3.2670, 3.3277, -0.0000, 2.9340, 3.2975, 3.2619,\n",
       "        3.2813, 3.4597, 3.8036, 3.0108, 3.2904, 3.2553, 3.2776, 3.5363, 4.1525,\n",
       "        5.0283, 3.8318, 3.3897, 3.2382, 3.4951, 4.2398, 5.3313, 6.5610, 4.8738,\n",
       "        3.7773, 3.3722, 4.0530, 5.3160, 6.6433, 8.0382, 6.1562, 4.6749, 3.7077,\n",
       "        4.8779, 6.5462, 8.0380, 9.5506])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dw = jacobian[3]\n",
    "df_dw_inv = torch.inverse(df_dw)\n",
    "\n",
    "R_grad_out = - torch.matmul(df_dw_inv, R_grad.grad)\n",
    "R_grad_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
